{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "text = \"아버지가 방에 들어갑니다.\"\n",
    "\n",
    "morphs = kkma.morphs(text)\n",
    "print(morphs)\n",
    "\n",
    "pos = kkma.pos(text)\n",
    "print(pos)\n",
    "\n",
    "nouns = kkma.nouns(text)\n",
    "print(nouns)\n",
    "\n",
    "sentences = \"오늘 날씨는 어때요? 내일은 덥다던데.\"\n",
    "s = kkma.sentences(sentences)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "\n",
    "text = \"아버지가 방에 들어갑니다.\"\n",
    "\n",
    "morphs = komoran.morphs(text)\n",
    "print(morphs)\n",
    "\n",
    "pos = komoran.pos(text)\n",
    "print(pos)\n",
    "\n",
    "nouns = komoran.nouns(text)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "text = \"서울의 기온은 매우 낮을 것입니다.\"\n",
    "\n",
    "morphs = okt.morphs(text)\n",
    "print(morphs)\n",
    "\n",
    "pos = okt.pos(text)\n",
    "print(pos)\n",
    "\n",
    "nouns = okt.nouns(text)\n",
    "print(nouns)\n",
    "\n",
    "text = \"와... 개쩐닼ㅋㅋㅋㅋ\"\n",
    "print(okt.normalize(text))\n",
    "print(okt.phrases(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran(userdic='./user_dic.tsv')\n",
    "text = \"우리 챗봇은 엔엘피를 좋아해\"\n",
    "pos = komoran.pos(text)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import numpy as np\n",
    "\n",
    "komoran = Komoran()\n",
    "text = \"오늘 날씨는 구름이 많아요.\"\n",
    "\n",
    "nouns = komoran.nouns(text)\n",
    "print(nouns)\n",
    "\n",
    "dics = {}\n",
    "for word in nouns:\n",
    "    if word not in dics.keys():\n",
    "        dics[word] = len(dics)\n",
    "print(dics)\n",
    "\n",
    "nb_classes = len(dics)\n",
    "targets = list(dics.values())\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n",
    "print(one_hot_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 말뭉치 데이터 읽기 시작\n",
      "200000\n",
      "1) 말뭉치 데이터 읽기 완료 :  1.673100471496582\n",
      "2) 형태소에서 명사만 추출 시작\n",
      "2) 형태소에서 명사만 추출 완료 :  87.36483502388\n",
      "3) Word2Vec 모델 학습 시작\n",
      "3) Word2Vec 모델 학습 완료 :  105.85037922859192\n",
      "4) 학습된 모델 저장 시작\n",
      "4) 학습된 모델 저장 완료 :  106.3192617893219\n",
      "corpus_count :  200000\n",
      "corpus_total_words :  1076896\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Komoran\n",
    "import time\n",
    "\n",
    "def read_review_data(filename):\n",
    "    with open(filename, 'r', encoding=\"UTF8\") as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print('1) 말뭉치 데이터 읽기 시작')\n",
    "review_data = read_review_data('./ratings.txt')\n",
    "print(len(review_data))\n",
    "print('1) 말뭉치 데이터 읽기 완료 : ', time.time() - start)\n",
    "\n",
    "\n",
    "print('2) 형태소에서 명사만 추출 시작')\n",
    "komoran = Komoran()\n",
    "docs = [komoran.nouns(sentence[1]) for sentence in review_data]\n",
    "print('2) 형태소에서 명사만 추출 완료 : ', time.time() - start)\n",
    "\n",
    "print('3) Word2Vec 모델 학습 시작')\n",
    "model = Word2Vec(sentences=docs, size=200, window=4, hs=1, min_count=2, sg=1)\n",
    "print('3) Word2Vec 모델 학습 완료 : ', time.time() - start)\n",
    "\n",
    "print('4) 학습된 모델 저장 시작')\n",
    "model.save('nvmc.model')\n",
    "print('4) 학습된 모델 저장 완료 : ', time.time() - start)\n",
    "\n",
    "print(\"corpus_count : \", model.corpus_count)\n",
    "print(\"corpus_total_words : \", model.corpus_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_total_words :  1076896\n",
      "사랑 :  [-2.18901098e-01 -2.30638266e-01 -3.23516726e-01 -1.10880509e-01\n",
      " -5.24657145e-02 -3.67247373e-01  2.49535859e-01 -1.10672571e-01\n",
      " -1.19900972e-01  5.35115078e-02 -4.20753807e-02 -1.63022757e-01\n",
      "  3.10032796e-02  5.87000251e-02  1.80825457e-01 -9.81938988e-02\n",
      "  1.56695947e-01  2.77327865e-01 -1.71590030e-01 -3.21360171e-01\n",
      " -1.89659074e-02  3.14504564e-01  1.14165783e-01 -9.37822089e-02\n",
      " -4.89308462e-02  1.42771974e-01  4.87611175e-01  2.64883310e-01\n",
      "  1.31326869e-01  6.54022321e-02 -5.31798825e-02  1.06777422e-01\n",
      " -3.57340509e-03  8.59801546e-02  2.35606715e-01 -1.58076987e-01\n",
      " -1.03374198e-02  1.60184503e-01 -5.67743219e-02 -1.14387959e-01\n",
      "  8.48006904e-02  1.62630752e-02  3.73258680e-01 -2.13345257e-03\n",
      "  1.42463073e-01 -3.05327505e-01 -4.06463087e-01 -9.46389064e-02\n",
      "  4.32475582e-02 -5.02498925e-01 -8.24670717e-02 -8.22420120e-02\n",
      " -1.53637022e-01  2.26479188e-01 -4.69100446e-01  4.85097691e-02\n",
      "  4.71878827e-01  1.51207283e-01  3.23728085e-01  1.01597108e-01\n",
      " -1.95750386e-01  1.86193481e-01 -3.19870800e-01  3.83708388e-01\n",
      "  2.13638037e-01 -1.16284043e-01 -2.05828428e-01  5.11621609e-02\n",
      "  2.84457564e-01 -1.56662643e-01 -3.30867708e-01 -8.49126950e-02\n",
      " -2.43772522e-01 -3.63577574e-01 -1.70495227e-01  1.03776492e-01\n",
      " -1.27607463e-02 -1.27620026e-01 -4.94027287e-02 -8.34354758e-02\n",
      " -3.70128155e-02 -2.16406241e-01 -2.92066664e-01 -1.34791620e-02\n",
      "  3.34517598e-01  9.53745991e-02  2.59860069e-01  6.94928244e-02\n",
      "  1.07402988e-01 -3.27572167e-01  3.35785262e-02  1.13541834e-01\n",
      "  1.37517780e-01  2.43552119e-01 -1.02563284e-01 -1.01520665e-01\n",
      "  1.22064367e-01 -1.39596105e-01  1.34276394e-02  1.53854797e-02\n",
      " -4.67230752e-03  1.27313375e-01 -2.46592805e-01 -1.96794406e-01\n",
      "  2.80651208e-02  1.66034758e-01  6.22787699e-02 -2.60984242e-01\n",
      " -1.42679047e-02  2.43712720e-02 -1.55180424e-01  4.61888701e-01\n",
      "  4.46382255e-05 -1.04882911e-01 -2.63523906e-01 -7.63684437e-02\n",
      "  2.56962448e-01 -3.21192831e-01  3.08848381e-01  1.62461415e-01\n",
      " -3.91210839e-02  1.10579856e-01  2.19921395e-01 -9.20721814e-02\n",
      "  3.10820788e-01 -3.99264783e-01  3.68805915e-01  9.43900943e-02\n",
      "  2.20943198e-01 -3.68393451e-01 -1.19109437e-01  1.34736523e-01\n",
      "  1.76278919e-01 -9.59831104e-02 -9.65517610e-02 -2.25214586e-02\n",
      "  6.06290437e-02  1.13153726e-01 -1.45290107e-01 -7.17325732e-02\n",
      "  1.07415833e-01 -1.63762420e-01 -2.93502212e-01 -9.64642614e-02\n",
      " -3.16201657e-01 -3.20466682e-02 -1.29419088e-01 -3.18060726e-01\n",
      "  1.80188149e-01  2.89177835e-01 -4.22630496e-02 -1.44796804e-01\n",
      " -2.88847238e-01  1.99350804e-01 -2.37252519e-01 -3.15522403e-01\n",
      "  2.27630496e-01 -1.62939390e-03 -3.98235649e-01  1.85395718e-01\n",
      "  9.63028818e-02 -7.66902789e-02 -2.18023703e-01  1.74231797e-01\n",
      " -8.22912380e-02 -6.50824681e-02  7.78316930e-02  1.42926857e-01\n",
      "  2.37267092e-01  9.61017236e-02  3.03372685e-02 -4.84025888e-02\n",
      "  2.28160441e-01 -2.74610996e-01 -6.05636165e-02  2.91774929e-01\n",
      "  4.64273505e-02 -3.32101822e-01  1.27709001e-01  3.28173377e-02\n",
      " -4.16574478e-01  1.40454143e-01  8.95205662e-02 -6.64288700e-02\n",
      "  3.02941710e-01 -1.56045958e-01  3.21986787e-02 -1.68491855e-01\n",
      "  8.31465945e-02  2.77149230e-02  2.01035142e-01  3.53935540e-01\n",
      "  2.69188851e-01  3.69968005e-02 -2.18224481e-01  1.86947539e-01\n",
      "  2.36695074e-02 -4.57510203e-02  1.91299453e-01  9.19692367e-02]\n",
      "일요일 = 월요일\t 0.6792425\n",
      "안성기 = 배우\t 0.58067536\n",
      "대기업 = 삼성\t 0.6054879\n",
      "일요일 != 삼성\t 0.2628131\n",
      "히어로 != 삼성\t 0.1367892\n",
      "[('씨야', 0.7127472162246704), ('장미희', 0.7094433307647705), ('고준희', 0.7018316984176636), ('김갑수', 0.7007349729537964), ('임원희', 0.6975858807563782), ('킬리언 머피', 0.6950335502624512), ('박중훈', 0.6948677897453308), ('정려원', 0.6947683095932007)]\n",
      "[('캐리비안의 해적', 0.6742691993713379), ('더 울버린', 0.6503077745437622), ('기사단', 0.644269585609436), ('엑스맨', 0.6423511505126953), ('데스티네이션', 0.6404320597648621), ('꽃보다 시리즈', 0.6390829682350159), ('X맨', 0.6337435245513916), ('러시아워', 0.6333252787590027)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load('nvmc.model')\n",
    "print(\"corpus_total_words : \", model.corpus_total_words)\n",
    "\n",
    "print('사랑 : ', model.wv['사랑'])\n",
    "\n",
    "print(\"일요일 = 월요일\\t\", model.wv.similarity(w1='일요일', w2='월요일'))\n",
    "print(\"안성기 = 배우\\t\", model.wv.similarity(w1='안성기', w2='배우'))\n",
    "print(\"대기업 = 삼성\\t\", model.wv.similarity(w1='대기업', w2='삼성'))\n",
    "print(\"일요일 != 삼성\\t\", model.wv.similarity(w1='일요일', w2='삼성'))\n",
    "print(\"히어로 != 삼성\\t\", model.wv.similarity(w1='히어로', w2='삼성'))\n",
    "\n",
    "print(model.wv.most_similar(\"안성기\", topn=8))\n",
    "print(model.wv.most_similar(\"시리즈\", topn=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '트리니티'), ('트리니티', '입학'), ('입학',))\n",
      "(('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '대학교'), ('대학교', '입학'), ('입학',))\n",
      "0.16666666666666666\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "def word_ngram(bow, num_gram):\n",
    "    text = tuple(bow)\n",
    "    ngrams = [text[x:x + num_gram] for x in range(0, len(text))]\n",
    "    return tuple(ngrams)\n",
    "\n",
    "def similarity(doc1, doc2):\n",
    "    cnt = 0\n",
    "    for token in doc1:\n",
    "        if token in doc2:\n",
    "            cnt = cnt + 1\n",
    "        return cnt/len(doc1)\n",
    "\n",
    "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.'\n",
    "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.'\n",
    "sentence3 = '나는 맛있는 밥을 뉴턴 선생님과 함께 먹었다.'\n",
    "\n",
    "komoran = Komoran()\n",
    "bow1 = komoran.nouns(sentence1)\n",
    "bow2 = komoran.nouns(sentence2)\n",
    "bow3 = komoran.nouns(sentence3)\n",
    "\n",
    "doc1 = word_ngram(bow1, 2)\n",
    "doc2 = word_ngram(bow2, 2)\n",
    "doc3 = word_ngram(bow3, 2)\n",
    "\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "\n",
    "r1 = similarity(doc1, doc2)\n",
    "r2 = similarity(doc3, doc1)\n",
    "\n",
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 1, '입학': 1, '대학교': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 0, '입학': 1, '대학교': 1, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "{'6월': 0, '뉴턴': 1, '선생님': 0, '제안': 0, '트리니티': 0, '입학': 0, '대학교': 0, '밥': 1, '선생': 1, '님과 함께': 1}\n",
      "0.8333333333333335\n",
      "0.20412414523193154\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def make_term_doc_mat(sentence_bow, word_dics):\n",
    "    freq_mat = {}\n",
    "    \n",
    "    for word in word_dics:\n",
    "        freq_mat[word] = 0\n",
    "        \n",
    "    for word in word_dics:\n",
    "        if word in sentence_bow:\n",
    "            freq_mat[word] += 1\n",
    "    \n",
    "    return freq_mat\n",
    "\n",
    "def make_vector(tdm):\n",
    "    vec = []\n",
    "    for key in tdm:\n",
    "        vec.append(tdm[key])\n",
    "    return vec\n",
    "\n",
    "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.'\n",
    "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.'\n",
    "sentence3 = '나는 맛있는 밥을 뉴턴 선생님과 함께 먹었다.'\n",
    "\n",
    "komoran = Komoran()\n",
    "bow1 = komoran.nouns(sentence1)\n",
    "bow2 = komoran.nouns(sentence2)\n",
    "bow3 = komoran.nouns(sentence3)\n",
    "\n",
    "bow = bow1 + bow2 + bow3\n",
    "\n",
    "word_dics = []\n",
    "for token in bow:\n",
    "    if token not in word_dics:\n",
    "        word_dics.append(token)\n",
    "\n",
    "freq_list1 = make_term_doc_mat(bow1, word_dics)\n",
    "freq_list2 = make_term_doc_mat(bow2, word_dics)\n",
    "freq_list3 = make_term_doc_mat(bow3, word_dics)\n",
    "print(freq_list1)\n",
    "print(freq_list2)\n",
    "print(freq_list3)\n",
    "\n",
    "doc1 = np.array(make_vector(freq_list1))\n",
    "doc2 = np.array(make_vector(freq_list2))\n",
    "doc3 = np.array(make_vector(freq_list3))\n",
    "\n",
    "r1 = cos_sim(doc1, doc2)\n",
    "r2 = cos_sim(doc3, doc1)\n",
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2100/2100 [==============================] - 2s 917us/step - loss: 1.2659 - accuracy: 0.6049 - val_loss: 0.3759 - val_accuracy: 0.8920\n",
      "Epoch 2/10\n",
      "2100/2100 [==============================] - 2s 890us/step - loss: 0.3726 - accuracy: 0.8937 - val_loss: 0.3170 - val_accuracy: 0.9064\n",
      "Epoch 3/10\n",
      "2100/2100 [==============================] - 2s 876us/step - loss: 0.2952 - accuracy: 0.9157 - val_loss: 0.2604 - val_accuracy: 0.9253\n",
      "Epoch 4/10\n",
      "2100/2100 [==============================] - 2s 870us/step - loss: 0.2629 - accuracy: 0.9258 - val_loss: 0.2385 - val_accuracy: 0.9314\n",
      "Epoch 5/10\n",
      "2100/2100 [==============================] - 2s 872us/step - loss: 0.2296 - accuracy: 0.9350 - val_loss: 0.2242 - val_accuracy: 0.9368\n",
      "Epoch 6/10\n",
      "2100/2100 [==============================] - 2s 857us/step - loss: 0.2098 - accuracy: 0.9397 - val_loss: 0.2053 - val_accuracy: 0.9417\n",
      "Epoch 7/10\n",
      "2100/2100 [==============================] - 2s 862us/step - loss: 0.1964 - accuracy: 0.9432 - val_loss: 0.1886 - val_accuracy: 0.9471\n",
      "Epoch 8/10\n",
      "2100/2100 [==============================] - 2s 882us/step - loss: 0.1769 - accuracy: 0.9487 - val_loss: 0.1841 - val_accuracy: 0.9472\n",
      "Epoch 9/10\n",
      "2100/2100 [==============================] - 2s 848us/step - loss: 0.1688 - accuracy: 0.9518 - val_loss: 0.1695 - val_accuracy: 0.9524\n",
      "Epoch 10/10\n",
      "2100/2100 [==============================] - 2s 881us/step - loss: 0.1627 - accuracy: 0.9525 - val_loss: 0.1643 - val_accuracy: 0.9532\n",
      "모델 평가\n",
      "313/313 [==============================] - 0s 604us/step - loss: 0.1714 - accuracy: 0.9500\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_10 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 20)                15700     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 16,330\n",
      "Trainable params: 16,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    " \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000)\n",
    "train_size = int(len(x_train) * 0.7)\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).batch(20)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#신경망 생성\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "#생선한 신경망을 학습\n",
    "hist = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "print('모델 평가')\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.save('mnist_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 20)                15700     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 16,330\n",
      "Trainable params: 16,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "313/313 [==============================] - 0s 393us/step - loss: 0.2028 - accuracy: 0.9402\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C717EC5F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "손글씨 이미지 예측값 :  [6]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, (x_test, y_test) = mnist.load_data()\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "model = load_model('mnist_model.h5')\n",
    "model.summary()\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "plt.imshow(x_test[100], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "picks = [100]\n",
    "predict = model.predict_classes(x_test[picks])\n",
    "print(\"손글씨 이미지 예측값 : \", predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "414/414 [==============================] - 7s 16ms/step - loss: 0.9938 - accuracy: 0.4627 - val_loss: 0.5985 - val_accuracy: 0.7885\n",
      "Epoch 2/5\n",
      "414/414 [==============================] - 6s 15ms/step - loss: 0.5813 - accuracy: 0.7740 - val_loss: 0.2923 - val_accuracy: 0.8981\n",
      "Epoch 3/5\n",
      "414/414 [==============================] - 6s 15ms/step - loss: 0.3348 - accuracy: 0.8891 - val_loss: 0.1549 - val_accuracy: 0.9564\n",
      "Epoch 4/5\n",
      "414/414 [==============================] - 6s 15ms/step - loss: 0.2047 - accuracy: 0.9350 - val_loss: 0.0952 - val_accuracy: 0.9746\n",
      "Epoch 5/5\n",
      "414/414 [==============================] - 6s 15ms/step - loss: 0.1307 - accuracy: 0.9595 - val_loss: 0.0720 - val_accuracy: 0.9746\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0553 - accuracy: 0.9873\n",
      "Accuracy: 98.730963 \n",
      "Loss: 0.055319\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "train_file = \"./chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "MAX_SEQ_LEN = 15\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size).take(test_size).batch(20)\n",
    "\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(word_index) + 1\n",
    "\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=4,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=5,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(3, name='logits')(dropout_hidden)\n",
    "\n",
    "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print('Accuracy: %f ' % (accuracy * 100))\n",
    "print('Loss: %f' % (loss))\n",
    "\n",
    "model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 15, 128)      1715072     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 15, 128)      0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 13, 128)      49280       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 12, 128)      65664       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 11, 128)      82048       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 128)          49280       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, 3)            387         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 3)            12          logits[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,961,743\n",
      "Trainable params: 1,961,743\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "100/100 - 0s - loss: 0.0702 - accuracy: 0.9785\n",
      "단어 시퀀스 :  ['1지망', '학교', '떨어졌어']\n",
      "단어 인덱스 시퀀스 :  [4648  343  448    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "문장 분류(정답) :  0\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C715BDF280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "감정 예측 점수 :  [[9.9927360e-01 5.8017357e-04 1.4628886e-04]]\n",
      "감정 예측 클래스 :  [0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "train_file = \"./chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "MAX_SEQ_LEN = 15\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "test_ds = ds.take(2000).batch(20)\n",
    "\n",
    "model = load_model('cnn_model.h5')\n",
    "model.summary()\n",
    "model.evaluate(test_ds, verbose=2)\n",
    "\n",
    "print(\"단어 시퀀스 : \", corpus[1])\n",
    "print(\"단어 인덱스 시퀀스 : \", padded_seqs[1])\n",
    "print(\"문장 분류(정답) : \", labels[1])\n",
    "\n",
    "picks = [1]\n",
    "predict = model.predict(padded_seqs[picks])\n",
    "predict_class = tf.math.argmax(predict, axis=1)\n",
    "print(\"감정 예측 점수 : \", predict)\n",
    "print(\"감정 예측 클래스 : \", predict_class.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
